---
title: "Qwen3 VL 30B A3B Instruct"
description: "> Qwen3 VL 30B A3B Instruct is a vision-language model designed for long-context multimodal reasoning, OCR, video understanding, and tool-based agent workflows."
---

## About the Provider
Qwen is an AI model family developed by Alibaba Group, a major Chinese technology and cloud computing company. Through its Qwen initiative, Alibaba builds and open-sources advanced language, images and coding models under permissive licenses to support innovation, developer tooling, and scalable AI integration across applications.

## Model Quickstart

This section helps you quickly get started with the `Qwen/Qwen3-VL-30B-A3B-Instruct` model on the Qubrid AI inferencing platform.

To use this model, you need:

- A valid **Qubrid API key**
- Access to the Qubrid inference API
- Basic knowledge of making API requests in your preferred language

Once authenticated with your API key, you can send inference requests to the `Qwen/Qwen3-VL-30B-A3B-Instruct` model and receive responses based on your input prompts.

Below are example placeholders showing how the model can be accessed using different programming environments.  
You can choose the one that best fits your workflow.

<CodeGroup>
  ```python Python theme={null}
import requests
import json
from pprint import pprint
url = "https://platform.qubrid.com/api/v1/qubridai/multimodal/chat"
headers = {
"Authorization": "Bearer Qubrid_API_KEY",
"Content-Type": "application/json"
}
data = {
"model": "Qwen/Qwen3-VL-30B-A3B-Instruct,
"messages": [
  {
    "role": "user",
    "content": [
      {
        "type": "text",
        "text": "What is in this image? Describe the main elements."
      },
      {
        "type": "image_url",
        "image_url": {
          "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
        }
      }
    ]
  }
],
"max_tokens": 4096,
"temperature": 0.7,
"stream": False,
}
response = requests.post(
    url,
    headers=headers,
    json=data,
    stream=True  
)
content_type = response.headers.get("Content-Type", "")
if "application/json" in content_type:
    pprint(response.json())
else:
    for line in response.iter_lines(decode_unicode=True):
        if not line:
            continue
        if line.startswith("data:"):
            payload = line.replace("data:", "").strip()
            if payload == "[DONE]":
                break
            try:
                chunk = json.loads(payload)
                pprint(chunk)
            except json.JSONDecodeError:
                print("Raw chunk:", payload)
```

```js JavaScript theme={null}
(async () => {
  const body = {
    model: "Qwen/Qwen3-VL-30B-A3B-Instruct",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "What is in this image? Describe the main elements.",
          },
          {
            type: "image_url",
            image_url: {
              url: "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
            },
          },
        ],
      },
    ],
    max_tokens: 4096,
    temperature: 0.7,
    stream: true,
  };

  const res = await fetch(
    "https://platform.qubrid.com/api/v1/qubridai/multimodal/chat",
    {
      method: "POST",
      headers: {
        Authorization:
        "Bearer Qubrid_API_KEY,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
    }
  );

  const contentType = res.headers.get("content-type") || "";
  if (contentType.includes("application/json")) {
    const result = await res.json();
    console.log(result);
  } else {
    const reader = res.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buffer = "";
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split("\n");
      buffer = lines.pop();
      for (const line of lines) {
        if (!line.startsWith("data:")) continue;
        const payload = line.replace("data:", "").trim();
        if (payload === "[DONE]") return;
        try {
          const chunk = JSON.parse(payload);
          console.log(chunk);
        } catch {
          console.log("Raw chunk:", payload);
        }
      }
    }
  }
})();
```

```go Go theme={null}
package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"
)

func main() {
	url := "https://platform.qubrid.com/api/v1/qubridai/multimodal/chat"

	data := map[string]interface{}{
		"model": "Qwen/Qwen3-VL-30B-A3B-Instruct",
		"messages": []interface{}{
			map[string]interface{}{
				"role": "user",
				"content": []interface{}{
					map[string]interface{}{
						"type": "text",
						"text": "What is in this image? Describe the main elements.",
					},
					map[string]interface{}{
						"type": "image_url",
						"image_url": map[string]interface{}{
							"url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
						},
					},
				},
			},
		},
		"max_tokens":  4096,
		"temperature": 0.7,
		"stream": true, 
	}

	jsonData, err := json.Marshal(data)
	if err != nil {
		panic(err)
	}

	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		panic(err)
	}

	req.Header.Set("Authorization", "Bearer Qubrid_API_KEY")
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	res, err := client.Do(req)
	if err != nil {
		panic(err)
	}
	defer res.Body.Close()
	contentType := res.Header.Get("Content-Type")
	if strings.Contains(contentType, "application/json") {
		var result map[string]interface{}
		if err := json.NewDecoder(res.Body).Decode(&result); err != nil {
			panic(err)
		}
		fmt.Printf("%+v\n", result)
		return
	}
	scanner := bufio.NewScanner(res.Body)
	for scanner.Scan() {
		line := scanner.Text()
		if !strings.HasPrefix(line, "data:") {
			continue
		}
		payload := strings.TrimSpace(strings.TrimPrefix(line, "data:"))
		if payload == "[DONE]" {
			break
		}
		var chunk map[string]interface{}
		if err := json.Unmarshal([]byte(payload), &chunk); err != nil {
			fmt.Println("Raw chunk:", payload)
			continue
		}
		fmt.Printf("%+v\n", chunk)
	}
	if err := scanner.Err(); err != nil {
		panic(err)
	}
}
```

```curl cURL theme={null}
curl -X POST "https://platform.qubrid.com/api/v1/qubridai/multimodal/chat" \
  -H "Authorization: Bearer <QUBRID_API_KEY>" \
  -H "Content-Type: application/json" \
  -d '{
  "model": "Qwen/Qwen3-VL-30B-A3B-Instruct",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image? Describe the main elements."
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
          }
        }
      ]
    }
  ],
  "max_tokens": 4096,
  "temperature": 0.7
}'
```

</CodeGroup>

## Model Overview
**Qwen3 VL 30B A3B Instruct** is a large-scale vision-language model designed to process and reason over both text and visual inputs. It combines strong text understanding with advanced visual perception, spatial reasoning, OCR, and video comprehension. The model supports long-context workloads and agent-style interactions, enabling it to operate GUIs, invoke tools, and complete multimodal tasks. It is provided in a Mixture-of-Experts (MoE) architecture and optimized for inference across edge-to-cloud environments. This Instruct variant is intended for instruction-following and multimodal interaction.

### Model at a Glance

| Feature            | Details                     |
| ------------------ | --------------------------- |
| Model ID           | `Qwen/Qwen3-VL-30B-A3B-Instruct`    |
| Provider           | Qwen                        |
| Architecture       | Transformer decoder-only (Qwen3-VL with ViT visual encoder)   |
| Model Size         | 9B                          |
| Parameters         | 6                           |
| Max Tokens         | 32K tokens                  |
| Image Input        | Supported                   |

### When to use?

You should consider using **Qwen3 VL 30B A3B Instruct** if:

- Your application requires both text and image understanding
- You need long-context processing for documents or videos
- Your workflows involve tool usage or agent-style interactions
- You need OCR and document parsing across multiple languages
- You require serverless multimodal inference

`This model is suitable for inference scenarios where multimodal comprehension and extended context handling are required.`

### Inference parameters

| Parameter Name     | Type    | Default | Description                                   |
|--------------------|---------|---------|-----------------------------------------------|
| Streaming          | boolean | true    | Enable streaming responses for real-time output. |
| Temperature        | number  | 0.7     | Controls randomness in the output.            |
| Max Tokens         | number  | 2048    | Maximum number of tokens to generate.         |
| Top P              | number  | 0.9     | Controls nucleus sampling.                    |
| Top K              | number  | 50      | Limits sampling to the top-k tokens.          |
| Presence Penalty   | number  | 0       | Discourages repeated tokens in the output.    |

### Key Features

- **Visual Agent Operations :**
Can operate PC and mobile GUIs by recognizing interface elements, understanding their functions, invoking tools, and completing tasks.

- **Advanced Visual and Spatial Reasoning :**
Judges object positions, viewpoints, occlusions, and supports stronger 2D grounding with enabled 3D grounding for spatial reasoning.

- **Long Context and Video Understanding :**
Supports a native 256K context window, expandable up to 1M, enabling processing of books and hours-long videos with second-level indexing.

- **Multimodal Reasoning :**
Combines visual and textual reasoning for causal analysis, logical problem-solving, and evidence-based responses, including STEM and math tasks.

- **Expanded OCR and Visual Recognition :**
Supports OCR across 32 languages and improves recognition under low light, blur, tilt, rare characters, and long-document structures.

### Summary
**Qwen3 VL 30B A3B Instruct** is a Mixture-of-Experts vision-language model designed for multimodal inference. 
- It processes text, images, and long-context inputs in a unified manner. The model supports visual reasoning, OCR, video understanding, and tool usage.
- It enables agent-style interactions such as GUI operation and task completion. Serverless inference is supported, while fine-tuning is not. 
